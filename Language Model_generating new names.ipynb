{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network - Random name generator\n",
    "\n",
    "Congratulations! You’re expecting a baby! Whether it’s a boy or a girl, you want your child’s name to be unique and very special. What you do is to go online and visit a whole bunch of \"best baby names\" list, or use random name pickers. However, these names have been used over and over by at least a few hundred others before. If this is going through your mind, then keep reading!\n",
    "\n",
    "The solution that I am offering here is to use deep learning. First, you should collect a list of few hundred favorite names like the ones I've included here (IranianNames.txt). To generate new \"cool\" names, we will build a character-level recurrent neural network (RNN) and train it on the dataset. After our language model learned the patterns, we ask it to generate names you have never seen before!\n",
    "\n",
    "Great! Let's begin by writing some functions.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1 - Loading and preprocessing data\n",
    "\n",
    "The following cell loads the dataset of Persian names and creates a list of unique characters. As you can see the total number of characters is 27 (a-z plus \"\\n\" newline character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of names: 8750, total number of unique characters: 27\n"
     ]
    }
   ],
   "source": [
    "with open('IranianNames.txt', 'r') as datafile :\n",
    "    names = datafile.read().lower()\n",
    "\n",
    "chars = list(set(names))\n",
    "\n",
    "num_names, vocab_size = len(names), len(chars)\n",
    "print('total number of names: %d, total number of unique characters: %d' % (num_names, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create one hash table (ch2idx, see cell below) to map each character ([\"\\n\", a-z]) to an index from 0-26, and a second hash table (idx2ch, see cell below) that maps each index back to its corresponding character. This will help us figure out what index corresponds to what character in the probability distribution output of the softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char to index:  {'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "index to char:  {0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "ch2idx = { ch:idx for idx, ch in enumerate(sorted(chars)) }\n",
    "idx2ch = { idx:ch for idx, ch in enumerate(sorted(chars)) }\n",
    "print('char to index: ', ch2idx)\n",
    "print('index to char: ', idx2ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Implementing useful functions\n",
    "\n",
    "In this section we are implementing some useful functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters (n_x, n_h, n_y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs :\n",
    "        n_x: input layer size\n",
    "        n_h: hidden layer size\n",
    "        n_y: output layer size\n",
    "    Outputs:\n",
    "        W_hx: Weight matrix from input to hidden layer, shape (n_h, n_x)\n",
    "        b_hx: Bias from input to hidden layer, shape (n_h, 1)\n",
    "        W_hh: Weight matrix from first hidden layer to second hidden layer, shape (n_h, n_h)\n",
    "        W_yh: Weight matrix from the second hidden layer to output layer, shape (n_y, n_h)\n",
    "        b_yh: Bias from the second hidden layer to the output layer, shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    W_hx = np.random.randn(n_h, n_x)*0.01 # input to hidden\n",
    "    b_hx = np.zeros((n_h, 1)) # hidden bias\n",
    "    W_hh = np.random.randn(n_h, n_h)*0.01 # hidden to hidden\n",
    "    W_yh = np.random.randn(n_y, n_h)*0.01 # hidden to output\n",
    "    b_yh = np.zeros((n_y, 1)) # output bias\n",
    "        \n",
    "    return W_hx, b_hx, W_hh, W_yh, b_yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## softmax function\n",
    "def softmax (x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward (a_prev, x, W_hx, b_hx, W_hh, W_yh, b_yh):\n",
    "    \"\"\"\n",
    "    Inputs :\n",
    "        x : input vector, shape (n_x, 1)\n",
    "        a_prev: previous hidden state, shape, (n_h, 1)\n",
    "        W_hx: Weight matrix from input to hidden layer\n",
    "        b_hx: Bias from input to hidden layer\n",
    "        W_hh: Weight matrix from first hidden layer to second hidden layer\n",
    "        W_yh: Weight matrix from the second hidden layer to output layer\n",
    "        b_yh: Bias from the second hidden layer to the output layer\n",
    "    Outputs:\n",
    "        a_next: next hidden state, shape (n_h, 1)\n",
    "        p_t: unnormalized log probabilities for next characters, shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    a_next = np.tanh(np.dot(W_hx, x) + np.dot(W_hh, a_prev) + b_hx) \n",
    "    p_t = softmax(np.dot(W_yh, a_next) + b_yh)  \n",
    "    \n",
    "    return a_next, p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward (X, Y, a0, W_hx, b_hx, W_hh, W_yh, b_yh, vocab_size=27):\n",
    "    \"\"\"\n",
    "    Inputs :\n",
    "        X: input matrix, shape (n_x, m)\n",
    "        Y: label vector, shape (m, 1)\n",
    "        a0: initial hidden state, shape (n_h, 1)\n",
    "        W_hx: Weight matrix from input to hidden layer\n",
    "        b_hx: Bias from input to hidden layer\n",
    "        W_hh: Weight matrix from first hidden layer to second hidden layer\n",
    "        W_yh: Weight matrix from the second hidden layer to output layer\n",
    "        b_yh: Bias from the second hidden layer to the output layer\n",
    "        vocab_size: number of vocabs, scalar-default is 27\n",
    "    Outputs:\n",
    "        cache tuple that includes:\n",
    "            y_hat: predicted labels, shape (m, 1)\n",
    "            a: hidden state dictionary\n",
    "            x: one-hot representation of X elements\n",
    "        loss: loss value, scalar\n",
    "    \"\"\"\n",
    "    # Initialize x, a and y_hat as empty dictionaries\n",
    "    x, a, y_hat = {}, {}, {}\n",
    "    \n",
    "    a[-1] = np.copy(a0)\n",
    "    loss = 0 # Initialize the loss\n",
    "    for t in range(len(X)):\n",
    "        x[t] = np.zeros((vocab_size,1)) \n",
    "        if (X[t] != None):\n",
    "            x[t][X[t]] = 1\n",
    "        \n",
    "        # Run one step forward of the RNN\n",
    "        a[t], y_hat[t] = rnn_step_forward(a[t-1], x[t], W_hx, b_hx, W_hh, W_yh, b_yh)\n",
    "        \n",
    "        # Update the loss by substracting the cross-entropy term of this time-step from it.\n",
    "        loss -= np.log(y_hat[t][Y[t],0])\n",
    "        \n",
    "    cache = (y_hat, a, x)\n",
    "\n",
    "    return loss, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward (dy, grads, W_hx, b_hx, W_hh, W_yh, b_yh, x, a, a_prev):\n",
    "    \"\"\"\n",
    "    Inputs :\n",
    "        dy: label error vector\n",
    "        grads: gradient dictionary\n",
    "        W_hx: Weight matrix from input to hidden layer\n",
    "        b_hx: Bias from input to hidden layer\n",
    "        W_hh: Weight matrix from first hidden layer to second hidden layer\n",
    "        W_yh: Weight matrix from the second hidden layer to output layer\n",
    "        b_yh: Bias from the second hidden layer to the output layer\n",
    "        x: one-hot representation of characters\n",
    "        a: hidden state, shape, (n_h, 1)\n",
    "        a_prev: previous hidden state, shape (n_h, 1)\n",
    "    Outputs:\n",
    "        grads: gradient dictionary\n",
    "    \"\"\"\n",
    "    grads['dWyh'] += np.dot(dy, a.T)\n",
    "    grads['dbyh'] += dy\n",
    "    da = np.dot(W_yh.T, dy) + grads['da_next'] # backpropagation into h\n",
    "    dummy = (1 - a * a) * da # backpropagation through tanh nonlinearity\n",
    "    grads['dbhx'] += dummy\n",
    "    grads['dWhx'] += np.dot(dummy, x.T)\n",
    "    grads['dWhh'] += np.dot(dummy, a_prev.T)\n",
    "    grads['da_next'] = np.dot(W_hh.T, dummy)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(X, Y, W_hx, b_hx, W_hh, W_yh, b_yh, cache):\n",
    "    \"\"\"\n",
    "    Inputs :\n",
    "        X: input matrix, shape (n_x, m)\n",
    "        Y: label vector, shape (m, 1)\n",
    "        a0: initial hidden state, shape, (n_h, 1)\n",
    "        W_hx: Weight matrix from input to hidden layer\n",
    "        b_hx: Bias from input to hidden layer\n",
    "        W_hh: Weight matrix from first hidden layer to second hidden layer\n",
    "        W_yh: Weight matrix from the second hidden layer to output layer\n",
    "        b_yh: Bias from the second hidden layer to the output layer\n",
    "        cache tuple that includes:\n",
    "            y_hat: predicted labels\n",
    "            a: hidden state dictionary\n",
    "            x: one-hot representation of X elements\n",
    "    Outputs:\n",
    "        grads: gradient dictionary\n",
    "        a: hidden state dictionary\n",
    "    \"\"\"\n",
    "    grads = {} # Initialize gradients as an empty dictionary\n",
    "    (y_hat, a, x) = cache # Retrieve from cache\n",
    "    \n",
    "    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n",
    "    grads['dWhx'], grads['dWhh'], grads['dWyh'] = np.zeros_like(W_hx), np.zeros_like(W_hh), np.zeros_like(W_yh)\n",
    "    grads['dbhx'], grads['dbyh'] = np.zeros_like(b_hx), np.zeros_like(b_yh)\n",
    "    grads['da_next'] = np.zeros_like(a[0])\n",
    "    \n",
    "    for t in reversed(range(len(X))): # reverse propagation through time\n",
    "        dy = np.copy(y_hat[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        grads = rnn_step_backward(dy, grads, W_hx, b_hx, W_hh, W_yh, b_yh, x[t], a[t], a[t-1])\n",
    "    \n",
    "    return grads, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W_hx, b_hx, W_hh, W_yh, b_yh, grads, lr):\n",
    "\n",
    "    W_hx += -lr * grads['dWhx']\n",
    "    W_hh += -lr * grads['dWhh']\n",
    "    W_yh += -lr * grads['dWyh']\n",
    "    b_hx  += -lr * grads['dbhx']\n",
    "    b_yh  += -lr * grads['dbyh']\n",
    "    \n",
    "    return W_hx, b_hx, W_hh, W_yh, b_yh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we will implement a function to prevent exploding of gradients and keep it between some range -N and N. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(grads, clip_val):\n",
    "    '''    \n",
    "    Inputs:\n",
    "        grads: a dictionary of the gradients \"dWhh\", \"dWhx\", \"dWyh\", \"dbhx\", \"dbyh\"\n",
    "        clip_val: gradients will be clipped between -clip_val and clip_val\n",
    "    Outputs:\n",
    "        grads: the dictionary of the gradients\n",
    "    '''\n",
    "    dWhh, dWhx, dWyh, dbhx, dbyh = grads['dWhh'], grads['dWhx'], grads['dWyh'], grads['dbhx'], grads['dbyh']\n",
    "   \n",
    "    for item in [dWhx, dWhh, dWyh, dbhx, dbyh]:\n",
    "        np.clip(item, -clip_val, clip_val, out=item)\n",
    "    \n",
    "    grads = {\"dWhh\": dWhh, \"dWhx\": dWhx, \"dWyh\": dWyh, \"dbhx\": dbhx, \"dbyh\": dbyh}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Building the language model \n",
    "\n",
    "Now let's build our character-level language model for text generation. The following function performs only one iteration of stochastic gradient descent (SGD) with clipped gradients according to the following steps:\n",
    "\n",
    "- Forward propagation through the RNN to compute the loss\n",
    "- Backward propagation through time to compute the gradients\n",
    "- Clip the gradients if necessary \n",
    "- Update synaptic weights using gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X, Y, a_prev, W_hx, b_hx, W_hh, W_yh, b_yh, lr=0.01):\n",
    "    \"\"\"    \n",
    "    Inputs :\n",
    "        X: input matrix, shape (n_x, m)\n",
    "        Y: label vector, shape (m, 1)\n",
    "        a_prev: previous hidden state, shape, (n_h, 1)\n",
    "        W_hx: Weight matrix from input to hidden layer\n",
    "        b_hx: Bias from input to hidden layer\n",
    "        W_hh: Weight matrix from first hidden layer to second hidden layer\n",
    "        W_yh: Weight matrix from the second hidden layer to output layer\n",
    "        b_yh: Bias from the second hidden layer to the output layer\n",
    "        learning_rate: learning rate, default 0.01\n",
    "    Outputs:\n",
    "        loss: value of the loss function (cross-entropy)\n",
    "        grads: a dictionary containing: dWhx, dWhh, dWyh, dbhx, dbyh\n",
    "        a: the last hidden state\n",
    "    \"\"\"\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, W_hx, b_hx, W_hh, W_yh, b_yh, vocab_size=27) # Forward propagation\n",
    "    grads, a = rnn_backward(X, Y, W_hx, b_hx, W_hh, W_yh, b_yh, cache) # Backpropagation\n",
    "    grads = clip(grads, 10) # clip gradients between -10 and 10\n",
    "    W_hx, b_hx, W_hh, W_yh, b_yh = update_parameters(W_hx, b_hx, W_hh, W_yh, b_yh, grads, lr) # Update parameters\n",
    "        \n",
    "    return loss, grads, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Training the model\n",
    "\n",
    "Now we train the model on the collected names. It is very important to shuffle the dataset, so that stochastic gradient descent visits the examples in random order. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(fname, idx2ch, ch2idx, num_iterations=100000, n_h=40, vocab_size=27):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        fname: name of the file\n",
    "        idx2ch: dictionary that maps the index to a character\n",
    "        ch2idx: dictionary that maps a character to an index\n",
    "        num_iterations: number of iterations to train the model for\n",
    "        n_h: number of hidden units of the RNN cell\n",
    "        vocab_size: size of the vocabulary\n",
    "    Outputs:\n",
    "        W_hx: Weight matrix from input to hidden layer, shape (n_h, n_x)\n",
    "        b_hx: Bias from input to hidden layer, shape (n_h, 1)\n",
    "        W_hh: Weight matrix from first hidden layer to second hidden layer, shape (n_h, n_h)\n",
    "        W_yh: Weight matrix from the second hidden layer to output layer, shape (n_y, n_h)\n",
    "        b_yh: Bias from the second hidden layer to the output layer, shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    n_x, n_y = vocab_size, vocab_size # Retrieve n_x and n_y from vocab_size    \n",
    "    W_hx, b_hx, W_hh, W_yh, b_yh = initialize_parameters(n_x, n_h, n_y) # Initialize parameters\n",
    "    \n",
    "    loss = -np.log(1.0/vocab_size) # Initialize loss\n",
    "    \n",
    "    with open(fname) as f:\n",
    "        names = f.readlines()\n",
    "    names = [x.lower().strip() for x in names]\n",
    "    np.random.shuffle(names) # Shuffle list of all names\n",
    "    \n",
    "    a_prev = np.zeros((n_h, 1)) # Initialize the hidden state\n",
    "    for ii in range(1, num_iterations):\n",
    "        idx = ii % len(names)\n",
    "        Y = [ch2idx[ch] for ch in names[idx]] + [char_to_ix[\"\\n\"]]\n",
    "        X = [None] + Y[:-1]\n",
    "        \n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        curr_loss, grads, a_prev = optimize(X, Y, a_prev, W_hx, b_hx, W_hh, W_yh, b_yh, lr=0.01)\n",
    "                \n",
    "        loss = loss * 0.999 + curr_loss * 0.001 # smoothing loss\n",
    "\n",
    "        if ii % 5000 == 0:\n",
    "            print('Iteration: %d, Loss: %f' % (ii, loss) + '\\n')\n",
    "        \n",
    "    return W_hx, b_hx, W_hh, W_yh, b_yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5000, Loss: 16.805458\n",
      "\n",
      "Iteration: 10000, Loss: 15.376390\n",
      "\n",
      "Iteration: 15000, Loss: 14.865807\n",
      "\n",
      "Iteration: 20000, Loss: 14.553933\n",
      "\n",
      "Iteration: 25000, Loss: 14.312119\n",
      "\n",
      "Iteration: 30000, Loss: 14.160799\n",
      "\n",
      "Iteration: 35000, Loss: 13.946571\n",
      "\n",
      "Iteration: 40000, Loss: 14.014759\n",
      "\n",
      "Iteration: 45000, Loss: 13.966208\n",
      "\n",
      "Iteration: 50000, Loss: 13.732016\n",
      "\n",
      "Iteration: 55000, Loss: 13.735051\n",
      "\n",
      "Iteration: 60000, Loss: 13.584354\n",
      "\n",
      "Iteration: 65000, Loss: 13.637023\n",
      "\n",
      "Iteration: 70000, Loss: 13.684620\n",
      "\n",
      "Iteration: 75000, Loss: 13.404673\n",
      "\n",
      "Iteration: 80000, Loss: 13.865663\n",
      "\n",
      "Iteration: 85000, Loss: 13.386959\n",
      "\n",
      "Iteration: 90000, Loss: 13.557052\n",
      "\n",
      "Iteration: 95000, Loss: 13.366332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W_hx, b_hx, W_hh, W_yh, b_yh = model('IranianNames.txt', idx2ch, ch2idx, num_iterations=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Sampling\n",
    "\n",
    "Now thw model is trained and it is able to generate new patterns. First, we feed the network a vector of zeros and carry out one step of forward propagation. Then, we randomly pick (see the next function) the next character's index according to the probability distribution specified by $\\hat{y}$. Then, the network is fed with picked index to generate another index until either the newline character is selected or this loop reaches a maximum value ($maxLen$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(W_hx, b_hx, W_hh, W_yh, b_yh, ch2idx, seed, maxLen=20):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        W_hx: Weight matrix from input to hidden layer, shape (n_h, n_x)\n",
    "        b_hx: Bias from input to hidden layer, shape (n_h, 1)\n",
    "        W_hh: Weight matrix from first hidden layer to second hidden layer, shape (n_h, n_h)\n",
    "        W_yh: Weight matrix from the second hidden layer to output layer, shape (n_y, n_h)\n",
    "        b_yh: Bias from the second hidden layer to the output layer, shape (n_y, 1)\n",
    "        ch2idx: dictionary that maps a character to an index\n",
    "        maxLen: maximum length of generated names\n",
    "    Outputs:\n",
    "        indices: a list containing the indices of the sampled characters\n",
    "    \"\"\"\n",
    "    vocab_size = b_yh.shape[0]\n",
    "    n_h = W_hh.shape[1]\n",
    "    \n",
    "    x = np.zeros((vocab_size, 1)) # one-hot vector of zeros as the initial input\n",
    "    a_prev = np.zeros((n_h, 1)) # initialize a previous hidden state\n",
    "    \n",
    "    indices = []\n",
    "    idx = -1 \n",
    "    newline_character = ch2idx['\\n']   \n",
    "    counter = 0\n",
    "    while (idx!=newline_character and counter!=maxLen):\n",
    "        \n",
    "        # Forward propagation\n",
    "        a = np.tanh(np.dot(W_hx, x) + np.dot(W_hh, a_prev) + b_hx)\n",
    "        y = softmax(np.dot(W_yh, a) + b_yh)\n",
    "        \n",
    "        # Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        np.random.seed(seed)\n",
    "        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n",
    "        \n",
    "        indices.append(idx) # Append the index to \"indices\"\n",
    "        \n",
    "        # Updating the input character with the sampled index.\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        a_prev = a # Update \"a_prev\" to be \"a\"\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "        \n",
    "    if (counter==maxLen) :\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jambodobe\n",
      "Ginube\n",
      "Goudi\n",
      "Joosbi\n",
      "Tasboar\n",
      "Bodoar\n",
      "Sasana\n",
      "Ayara\n",
      "Sana\n",
      "Asakhar\n",
      "Ohanan\n",
      "Ainou\n",
      "Ashid\n",
      "Pareeh\n",
      "Izare\n",
      "Sale\n",
      "Bahanahatah\n",
      "Bidi\n",
      "Maha\n",
      "Aralaza\n",
      "Kamiyl\n",
      "Alitl\n",
      "Anuz\n",
      "Izladeh\n",
      "Shahrourb\n",
      "Seiroude\n",
      "Behrokha\n",
      "Gorooz\n",
      "Mipand\n",
      "Shardad\n",
      "Marhanel\n",
      "Bolahen\n",
      "Sabanlbila\n",
      "Bahpour\n",
      "Anozan\n",
      "Goudan\n",
      "Mongofe\n",
      "Shahilarshad\n",
      "Firooqey\n",
      "Jalabele\n",
      "Ghadiza\n",
      "Bahrye\n",
      "Farzib\n",
      "Avve\n",
      "Ssii\n",
      "Yila\n",
      "Pabandir\n",
      "Aalehan\n",
      "Anousha\n",
      "Behroo\n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "seed = 0\n",
    "for ii in range(n):\n",
    "    # Sample indices and print them\n",
    "    indices = sample(W_hx, b_hx, W_hh, W_yh, b_yh, ch2idx, seed)\n",
    "    seed += 1\n",
    "    txt = ''.join(idx2ch[idx] for idx in indices)\n",
    "    txt = txt[0].upper() + txt[1:]  # capitalize first character \n",
    "    print ('%s' % (txt, ), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "You can see that the language model proposed here is very easy to implement while it is very good in generating meaningful patterns of characters. At the beginning of training, it was generating random patterns, but after training it generates some good names. However, not all the names are cool or meaningful. Please feel free to tweak parameters to see if you can get even better names.\n",
    "\n",
    "Obviously, the larger and more diverse training dataset would lead to more meaningful and novel patterns through improvisation. Training on a small dataset, like the one we used here, can cause the model to overfit which leads to generating many of the previously seen names and limited improvisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "\n",
    "- This notebook is inspired from Sequence Model course taugh by Andrew Ng: https://www.coursera.org/learn/nlp-sequence-models. To learn more about text generation, also check out Karpathy's [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
